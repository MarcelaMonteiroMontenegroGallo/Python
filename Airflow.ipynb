{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPot7SD5gFYOqv2bNG9jJTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcelaMonteiroMontenegroGallo/Python/blob/master/Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZPdsQBuhhxv"
      },
      "source": [
        "from airflow.models import DAG\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DsrkXwKwdkW"
      },
      "source": [
        "** DAG definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVhNuKT8agNk"
      },
      "source": [
        "etl_dag = DAG(\n",
        "    dag_id = 'etl_pipeline',\n",
        "    default_args={\"start_date\":2020-01-08\"}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLIWxqyjiv8g"
      },
      "source": [
        "# Define the default_args dictionary\n",
        "default_args = {\n",
        "  'owner': 'dsmith',\n",
        "  'start_date': datetime(2020, 1, 14),\n",
        "  'retries': 2\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZll2HPvry69"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjEZXz8xq4Xz"
      },
      "source": [
        "**BashOperator**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nmxLC00rkKu"
      },
      "source": [
        "from airflow.operators.bash_operator import BashOperator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWKwkXvbq3K9"
      },
      "source": [
        "BashOperator(\n",
        "    task_id='bash_exemaplo',\n",
        "    bash_command ='runcleanup.sh',\n",
        "    dag=ml_dag\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IOTfqmV6abbw"
      },
      "source": [
        "bash_task = BashOperator (task_id= 'clean_addresses',\n",
        "                          bash_command= 'cat addresses.txt'| awk =\"NF==10\" > cleaned.txt, \n",
        "                          dag=dag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrgEr1suGXl"
      },
      "source": [
        "# Import the BashOperator\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "\n",
        "# Define the BashOperator \n",
        "cleanup = BashOperator(\n",
        "    task_id='cleanup_task',\n",
        "    # Define the bash_command\n",
        "    bash_command='cleanup.sh',\n",
        "    # Add the task to the dag\n",
        "    dag=analytics_dag\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEKOJjkyKBMi"
      },
      "source": [
        "# Define a second operator to run the `consolidate_data.sh` script\n",
        "consolidate = BashOperator(\n",
        "    task_id='consolidate_task',\n",
        "    bash_command='consolidate_data.sh',\n",
        "    dag=analytics_dag)\n",
        "\n",
        "# Define a final operator to execute the `push_data.sh` script\n",
        "push_data = BashOperator(\n",
        "    task_id='pushdata_task',\n",
        "    bash_command='push_data.sh',\n",
        "    dag=analytics_dag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXlujRIQ20d7"
      },
      "source": [
        "from airflow.operators.python_operator import PythonOperator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9JevxWsgD1G"
      },
      "source": [
        "def printme():\n",
        "  print(\"This goes in the logs!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iewk1yh3fML"
      },
      "source": [
        "**op_kwargs** example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZADtiHv3c96"
      },
      "source": [
        "def sleep(length_of_time):\n",
        "  time.sleep(length_of_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRiKX6jk34kx"
      },
      "source": [
        "sleep_task = PythonOperator(\n",
        "    task_id='sleep',\n",
        "    python_callable= sleep,\n",
        "    op_kwargs={'length_of_time':5}\n",
        "    dag=example_dag\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaBuWG4U4jKM"
      },
      "source": [
        "**Email Operator**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MI3Zdsu4ng2"
      },
      "source": [
        "from airflow.operators.email_operator import EmailOperator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYibb4yx42xB"
      },
      "source": [
        "email_task =EmailOperator (\n",
        "    task_id ='email_sales_report',\n",
        "    to='sales_manager@example.com',\n",
        "    subject ='Automated Sales Report',\n",
        "    html_content ='Attached is lastest sales report',\n",
        "    files ='latest_sales.xlsx',\n",
        "    dag= example_dag\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJhDcjL74YB"
      },
      "source": [
        "def pull_file(URL, savepath):\n",
        "    r = requests.get(URL)\n",
        "    with open(savepath, 'wb') as f:\n",
        "        f.write(r.content)   \n",
        "    # Use the print method for logging\n",
        "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
        "\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "\n",
        "# Create the task\n",
        "pull_file_task = PythonOperator(\n",
        "    task_id='pull_file',\n",
        "    # Add the callable\n",
        "    python_callable=pull_file,\n",
        "    # Define the arguments\n",
        "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
        "    dag=process_sales_dag\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXN2InyeA6u_"
      },
      "source": [
        "**Airflow Scheduling**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHD-wAKFA_92"
      },
      "source": [
        "# Update the scheduling arguments as defined\n",
        "default_args = {\n",
        "  'owner': 'Engineering',\n",
        "  'start_date': datetime(2019, 11, 1),\n",
        "  'email': ['airflowresults@datacamp.com'],\n",
        "  'email_on_failure': False,\n",
        "  'email_on_retry': False,\n",
        "  'retries': 3,\n",
        "  'retry_delay': timedelta(minutes=20)\n",
        "}\n",
        "\n",
        "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP5Ap2kIb9Yy"
      },
      "source": [
        "**Sensor** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mF3mp0Acj_S"
      },
      "source": [
        "Other sensors \n",
        "\n",
        "ExternalTaskSensor -waint for a task in another DAG to Complete\n",
        "\n",
        "HttpSensor - Request a web URL and Check for content \n",
        "\n",
        "SqlSensor - Runs a SQL query to check for content \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSefK7S2cjha"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekv44Wmlb87c"
      },
      "source": [
        "from airflow.models import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.http_operator import SimpleHttpOperator\n",
        "from airflow.contrib.sensors.file_sensor import FileSensor\n",
        "\n",
        "dag = DAG(\n",
        "   dag_id = 'update_state',\n",
        "   default_args={\"start_date\": \"2019-10-01\"}\n",
        ")\n",
        "\n",
        "precheck = FileSensor(\n",
        "   task_id='check_for_datafile',\n",
        "   filepath='salesdata_ready.csv',\n",
        "   dag=dag)\n",
        "\n",
        "part1 = BashOperator(\n",
        "   task_id='generate_random_number',\n",
        "   bash_command='echo $RANDOM',\n",
        "   dag=dag\n",
        ")\n",
        "\n",
        "import sys\n",
        "def python_version():\n",
        "   return sys.version\n",
        "\n",
        "part2 = PythonOperator(\n",
        "   task_id='get_python_version',\n",
        "   python_callable=python_version,\n",
        "   dag=dag)\n",
        "   \n",
        "part3 = SimpleHttpOperator(\n",
        "   task_id='query_server_for_external_ip',\n",
        "   endpoint='https://api.ipify.org',\n",
        "   method='GET',\n",
        "   dag=dag)\n",
        "   \n",
        "precheck >> part3 >> part2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_WbeYQ7ey3x"
      },
      "source": [
        "**EXECUTOR **\n",
        "\n",
        "Executors run Taks \n",
        "Different executors handle running the task differently\n",
        "\n",
        "\n",
        "Exemple executors \n",
        "- Sequential Executor \n",
        "- Local Executor\n",
        "- Celery Executor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InIV89gjeymj"
      },
      "source": [
        "cat airflow/airflow.cfg | grep \"executor = \"executor = SequencialExecutor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjuc0bongozD"
      },
      "source": [
        "airflow list_dags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klwvMopcgyuK"
      },
      "source": [
        "INFO - Using SequentialExecutor "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IZX6gjfidyo"
      },
      "source": [
        "from airflow.models import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.contrib.sensors.file_sensor import FileSensor\n",
        "from datetime import datetime\n",
        "\n",
        "report_dag = DAG(\n",
        "    dag_id = 'execute_report',\n",
        "    schedule_interval = \"0 0 * * *\"\n",
        ")\n",
        "\n",
        "precheck = FileSensor(\n",
        "    task_id='check_for_datafile',\n",
        "    filepath='salesdata_ready.csv',\n",
        "    start_date=datetime(2020,2,20),\n",
        "    mode='reschedule',\n",
        "    dag=report_dag\n",
        ")\n",
        "\n",
        "generate_report_task = BashOperator(\n",
        "    task_id='generate_report',\n",
        "    bash_command='generate_report.sh',\n",
        "    start_date=datetime(2020,2,20),\n",
        "    dag=report_dag\n",
        ")\n",
        "\n",
        "precheck >> generate_report_task\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRHliZuhBZ-l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7MvfHSYBZ7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2kkBuPyA0Fm"
      },
      "source": [
        "**Debugging and troublesshooting in Airflow**\n",
        "\n",
        "\n",
        "1.   DAG won't run on Schedule\n",
        "- At leat one schedule_interval hasn't passed \n",
        "-- Modify the atributes to meet your requirements \n",
        "- Not enougt task free within the executor to run\n",
        "-- Change executor type \n",
        "-- Add system resources\n",
        "-- Add more systems\n",
        "-- Change DAG Scheduling\n",
        "\n",
        "\n",
        "2.   DAG won't load \n",
        "- DAG not in web UI\n",
        "- DAG not In airflow list_dags\n",
        "-- Possible solutions \n",
        "- Verify DAG file is in correct folder\n",
        "- Determine the DAGS folder via airflow.cfg\n",
        "-Note, the folder must be an absolute path \n",
        "\n",
        "\n",
        "\n",
        "3.   Syntax errors \n",
        "-The most common reason a DAG file won't appear \n",
        "- Sometimes difficult to find errors in Dag\n",
        "-- Run airflow list_dags\n",
        "-- Run python3 <dagfile.py>\n",
        "- \n",
        "- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGXfDipDGG6p"
      },
      "source": [
        "**Defining SLAS**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGj2El9zAzwV"
      },
      "source": [
        "task1 = BashOperator(task_id = 'sla_task',\n",
        "                     bash_command = 'runcode.sh',\n",
        "                     sla =timedelta(seconds =30),\n",
        "                     dag =dag)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}